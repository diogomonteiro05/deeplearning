{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Libraries\n",
    "Code to install the required libraries (diffusers, torch, torchvision, matplotlib) and import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install diffusers torch torchvision matplotlib\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset\n",
    "Define transformations and create a dataset using ImageFolder with the specified dataroot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Set the dataset root directory\n",
    "dataroot = \"data/wiki\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageFolder(root=dataroot, transform=transform)\n",
    "\n",
    "# Check the number of samples in the dataset\n",
    "print(f\"Number of images in the dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data Loader\n",
    "Initialize the DataLoader with parameters like batch_size and shuffle for batching the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize DataLoader\n",
    "batch_size = 16  # Define batch size\n",
    "shuffle = True   # Shuffle the dataset for training\n",
    "\n",
    "# Create DataLoader for batching\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Check the number of batches\n",
    "num_batches = len(dataloader)\n",
    "print(f\"Number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Diffusion Model and Trainer\n",
    "Instantiate the UNet2DModel, set up the noise scheduler, and define the optimizer and initial training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the diffusion model\n",
    "model = UNet2DModel(\n",
    "    sample_size=64,  # Image size\n",
    "    in_channels=3,   # Number of input channels (RGB)\n",
    "    out_channels=3,  # Number of output channels (RGB)\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the noise scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 15  # Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Implement the loop to iterate over data batches, add noise, predict noise, and update model weights. Note: This section appears twice in the notebook, but only one implementation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for step, (images, _) in enumerate(dataloader):\n",
    "        # Move images to the device\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(images).to(device)\n",
    "\n",
    "        # Sample random timesteps\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (images.shape[0],), device=device).long()\n",
    "\n",
    "        # Add noise to the images\n",
    "        noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "        # Predict the noise\n",
    "        noise_pred = model(noisy_images, timesteps).sample\n",
    "\n",
    "        # Compute loss (mean squared error)\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{len(dataloader)}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Display Images\n",
    "After training, set the model to evaluation mode, generate images using the reverse diffusion process, and display them using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display images after training using the correct de-noising loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_images = 16  # Total images to generate\n",
    "    rows, cols = 4, 4  # 4x4 grid\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    axes = axes.flatten()  # Flatten the array to iterate easily\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Start from random noise with the same size as training images (64x64 with 3 channels)\n",
    "        noisy_image = torch.randn(1, 3, 64, 64).to(device)\n",
    "        \n",
    "        # Reverse diffusion process\n",
    "        for t in reversed(range(noise_scheduler.num_train_timesteps)):\n",
    "            # Get noise prediction from the model\n",
    "            noise_pred = model(noisy_image, t).sample  \n",
    "            # Perform a de-noising step using the predicted noise\n",
    "            step_output = noise_scheduler.step(noise_pred, t, noisy_image)\n",
    "            noisy_image = step_output.prev_sample\n",
    "        \n",
    "        # Denormalize and prepare image for display\n",
    "        generated_image = (noisy_image.squeeze().cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5).clip(0, 1)\n",
    "        axes[i].imshow(generated_image)\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "import torch.nn as nn\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.models.inception import Inception_V3_Weights\n",
    "\n",
    "class InceptionV3FeatureExtractor:\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        weights = Inception_V3_Weights.DEFAULT\n",
    "        self.inception = inception_v3(weights=weights)\n",
    "        self.inception.eval()\n",
    "        self.inception.fc = nn.Identity()\n",
    "        self.inception.to(device)\n",
    "        self.preprocess = weights.transforms()\n",
    "\n",
    "    def extract_features(self, images):\n",
    "        features = []\n",
    "        with torch.no_grad():\n",
    "            for img in images:\n",
    "                # If img is a tensor of shape (3, H, W), convert it to PIL image via the official transforms\n",
    "                inp = self.preprocess(img).unsqueeze(0).to(self.device)\n",
    "                feature = self.inception(inp)\n",
    "                features.append(feature.cpu().numpy())\n",
    "        return np.concatenate(features, axis=0)\n",
    "\n",
    "def calculate_fid(real_features, fake_features):\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    sigma_real = np.cov(real_features, rowvar=False)\n",
    "    mu_fake = np.mean(fake_features, axis=0)\n",
    "    sigma_fake = np.cov(fake_features, rowvar=False)\n",
    "    mean_diff_squared = np.sum((mu_real - mu_fake) ** 2)\n",
    "    covmean = linalg.sqrtm(sigma_real.dot(sigma_fake))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    trace_term = np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    fid = mean_diff_squared + trace_term\n",
    "    return fid\n",
    "\n",
    "def generate_fake_image(model, noise_scheduler, device):\n",
    "    with torch.no_grad():\n",
    "        # Start from random noise (1, 3, 64, 64)\n",
    "        noisy_image = torch.randn(1, 3, 64, 64).to(device)\n",
    "        # Reverse diffusion process\n",
    "        for t in reversed(range(noise_scheduler.num_train_timesteps)):\n",
    "            noise_pred = model(noisy_image, t).sample\n",
    "            step_output = noise_scheduler.step(noise_pred, t, noisy_image)\n",
    "            noisy_image = step_output.prev_sample\n",
    "    # Output image in same scale as training images (assumed normalized to [-1, 1])\n",
    "    return noisy_image.squeeze(0)\n",
    "\n",
    "def compute_fid(real_imgs, model, noise_scheduler, device, num_samples=100):\n",
    "    feature_extractor = InceptionV3FeatureExtractor(device)\n",
    "    \n",
    "    # Extract features for real images\n",
    "    real_features = []\n",
    "    # Convert real_imgs [N, 3, 64, 64] into list of images\n",
    "    for img in real_imgs:\n",
    "        real_features.append(feature_extractor.extract_features([img]))\n",
    "    real_features = np.concatenate(real_features, axis=0)\n",
    "    \n",
    "    # Generate fake images and extract features\n",
    "    fake_features = []\n",
    "    for i in range(num_samples):\n",
    "        fake_img = generate_fake_image(model, noise_scheduler, device)\n",
    "        # Denormalize image if needed (assumes image was generated in same [-1,1] range)\n",
    "        fake_img = (fake_img * 0.5 + 0.5).clamp(0,1)\n",
    "        fake_features.append(feature_extractor.extract_features([fake_img]))\n",
    "    fake_features = np.concatenate(fake_features, axis=0)\n",
    "    \n",
    "    fid_score = calculate_fid(real_features, fake_features)\n",
    "    return fid_score\n",
    "\n",
    "# Prepare real images from your dataloader (using the same dataloader from earlier)\n",
    "def get_all_real_images(dataloader, max_imgs=1000):\n",
    "    all_images = []\n",
    "    for _, (imgs, _) in enumerate(dataloader):\n",
    "        all_images.append(imgs)\n",
    "        if len(torch.cat(all_images, dim=0)) >= max_imgs:\n",
    "            break\n",
    "    return torch.cat(all_images, dim=0)[:max_imgs]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Collecting real images for FID evaluation...\")\n",
    "real_imgs = get_all_real_images(dataloader)\n",
    "\n",
    "print(\"Computing FID score for diffusion model...\")\n",
    "fid_score = compute_fid(real_imgs, model, noise_scheduler, device, num_samples=min(100, len(real_imgs)))\n",
    "\n",
    "print(f\"Fréchet Inception Distance (FID): {fid_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
