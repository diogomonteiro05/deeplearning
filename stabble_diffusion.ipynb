{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Libraries\n",
    "To begin, we install the required libraries and import the necessary modules for building and training the diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install diffusers torch torchvision matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.models.inception import Inception_V3_Weights\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "We preprocess the dataset by resizing all images to 64x64 pixels, normalizing them to the range [-1, 1], and applying transformations to prepare them for training.\n",
    "\n",
    "## Dataset Creation\n",
    "\n",
    "We use the ImageFolder class to load the dataset from the specified directory and apply the defined transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Set the dataset root directory\n",
    "dataroot = \"data/wiki\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageFolder(root=dataroot, transform=transform)\n",
    "\n",
    "# Check the number of samples in the dataset\n",
    "print(f\"Number of images in the dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data Loader\n",
    "\n",
    "## Data Batching\n",
    "\n",
    "We initialize a DataLoader to batch the dataset for training. The batch_size parameter defines the number of images per batch, and shuffle=True ensures the data is shuffled for better training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataLoader\n",
    "batch_size = 16  # Define batch size\n",
    "shuffle = True   # Shuffle the dataset for training\n",
    "\n",
    "# Create DataLoader for batching\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Check the number of batches\n",
    "num_batches = len(dataloader)\n",
    "print(f\"Number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Diffusion Model and Trainer\n",
    "\n",
    "## Model Architecture\n",
    "We define the diffusion model using the UNet2DModel from the Hugging Face Diffusers library. The model follows a U-Net architecture with hierarchical feature extraction and attention mechanisms for better image generation.\n",
    "\n",
    "## Noise Scheduler\n",
    "The DDPMScheduler is used to manage the forward and reverse diffusion processes, with 1000 diffusion steps.\n",
    "\n",
    "## Optimizer\n",
    "We use the AdamW optimizer with a learning rate of 1e-4 to train the model.\n",
    "\n",
    "## Device Setup\n",
    "The model is moved to the appropriate device (GPU if available, otherwise CPU) for efficient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the diffusion model\n",
    "model = UNet2DModel(\n",
    "    sample_size=64,  # Image size\n",
    "    in_channels=3,   # Number of input channels (RGB)\n",
    "    out_channels=3,  # Number of output channels (RGB)\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the noise scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 15  # Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop \n",
    "\n",
    "## Training Strategy\n",
    "\n",
    "The training loop iterates over the dataset for a specified number of epochs. For each batch:\n",
    "1. Gaussian noise is added to the images to simulate the forward diffusion process.\n",
    "2. The model predicts the added noise.\n",
    "3. The loss is computed using Mean Squared Error (MSE) between the predicted and actual noise.\n",
    "4. Backpropagation is performed to update the model's weights using the AdamW optimizer.\n",
    "5. Progress is monitored by printing the loss every 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for step, (images, _) in enumerate(dataloader):\n",
    "        # Move images to the device\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(images).to(device)\n",
    "\n",
    "        # Sample random timesteps\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (images.shape[0],), device=device).long()\n",
    "\n",
    "        # Add noise to the images\n",
    "        noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "        # Predict the noise\n",
    "        noise_pred = model(noisy_images, timesteps).sample\n",
    "\n",
    "        # Compute loss (mean squared error)\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{len(dataloader)}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Display Images\n",
    "\n",
    "## Image Generation\n",
    "After training, the model is set to evaluation mode. We generate images by starting with random noise and applying the reverse diffusion process step-by-step.\n",
    "\n",
    "## Visualization\n",
    "The generated images are denormalized and displayed in a 4x4 grid using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display images after training using the correct de-noising loop\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_images = 16  # Total images to generate\n",
    "    rows, cols = 4, 4  # 4x4 grid\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    axes = axes.flatten() \n",
    "    \n",
    "    for i in range(num_images):\n",
    "        noisy_image = torch.randn(1, 3, 64, 64).to(device)\n",
    "        \n",
    "        # Reverse diffusion process\n",
    "        for t in reversed(range(noise_scheduler.num_train_timesteps)):\n",
    "            # Get noise prediction from the model\n",
    "            noise_pred = model(noisy_image, t).sample  \n",
    "            # Perform a de-noising step using the predicted noise\n",
    "            step_output = noise_scheduler.step(noise_pred, t, noisy_image)\n",
    "            noisy_image = step_output.prev_sample\n",
    "        \n",
    "        # Denormalize and prepare image for display\n",
    "        generated_image = (noisy_image.squeeze().cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5).clip(0, 1)\n",
    "        axes[i].imshow(generated_image)\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID Metric\n",
    "\n",
    "The Fréchet Inception Distance (FID) is used to evaluate the quality of the generated images by comparing their features to those of real images.\n",
    "\n",
    "## Feature Extraction\n",
    "We use an InceptionV3 model to extract features from both real and generated images.\n",
    "\n",
    "## FID Calculation\n",
    "The FID score is computed based on the mean and covariance of the features from real and generated images. A lower FID score indicates better image quality and diversity.\n",
    "\n",
    "## Real Image Collection\n",
    "We collect real images from the dataset using the DataLoader for FID evaluation.\n",
    "\n",
    "## FID Computation\n",
    "The FID score is calculated for the diffusion model, providing a quantitative measure of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3FeatureExtractor:\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        weights = Inception_V3_Weights.DEFAULT\n",
    "        self.inception = inception_v3(weights=weights)\n",
    "        self.inception.eval()\n",
    "        self.inception.fc = nn.Identity()\n",
    "        self.inception.to(device)\n",
    "        self.preprocess = weights.transforms()\n",
    "\n",
    "    def extract_features(self, images):\n",
    "        features = []\n",
    "        with torch.no_grad():\n",
    "            for img in images:\n",
    "                inp = self.preprocess(img).unsqueeze(0).to(self.device)\n",
    "                feature = self.inception(inp)\n",
    "                features.append(feature.cpu().numpy())\n",
    "        return np.concatenate(features, axis=0)\n",
    "\n",
    "def calculate_fid(real_features, fake_features):\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    sigma_real = np.cov(real_features, rowvar=False)\n",
    "    mu_fake = np.mean(fake_features, axis=0)\n",
    "    sigma_fake = np.cov(fake_features, rowvar=False)\n",
    "    mean_diff_squared = np.sum((mu_real - mu_fake) ** 2)\n",
    "    covmean = linalg.sqrtm(sigma_real.dot(sigma_fake))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    trace_term = np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    fid = mean_diff_squared + trace_term\n",
    "    return fid\n",
    "\n",
    "def generate_fake_image(model, noise_scheduler, device):\n",
    "    with torch.no_grad():\n",
    "        noisy_image = torch.randn(1, 3, 64, 64).to(device)\n",
    "        for t in reversed(range(noise_scheduler.num_train_timesteps)):\n",
    "            noise_pred = model(noisy_image, t).sample\n",
    "            step_output = noise_scheduler.step(noise_pred, t, noisy_image)\n",
    "            noisy_image = step_output.prev_sample\n",
    "    return noisy_image.squeeze(0)\n",
    "\n",
    "def compute_fid(real_imgs, model, noise_scheduler, device, num_samples=100):\n",
    "    feature_extractor = InceptionV3FeatureExtractor(device)\n",
    "    \n",
    "    # Extract features for real images\n",
    "    real_features = []\n",
    "    for img in real_imgs:\n",
    "        real_features.append(feature_extractor.extract_features([img]))\n",
    "    real_features = np.concatenate(real_features, axis=0)\n",
    "    \n",
    "    # Generate fake images and extract features\n",
    "    fake_features = []\n",
    "    for i in range(num_samples):\n",
    "        fake_img = generate_fake_image(model, noise_scheduler, device)\n",
    "        fake_img = (fake_img * 0.5 + 0.5).clamp(0,1)\n",
    "        fake_features.append(feature_extractor.extract_features([fake_img]))\n",
    "    fake_features = np.concatenate(fake_features, axis=0)\n",
    "    \n",
    "    fid_score = calculate_fid(real_features, fake_features)\n",
    "    return fid_score\n",
    "\n",
    "# Prepare real images from your dataloader (using the same dataloader from earlier)\n",
    "def get_all_real_images(dataloader, max_imgs=1000):\n",
    "    all_images = []\n",
    "    for _, (imgs, _) in enumerate(dataloader):\n",
    "        all_images.append(imgs)\n",
    "        if len(torch.cat(all_images, dim=0)) >= max_imgs:\n",
    "            break\n",
    "    return torch.cat(all_images, dim=0)[:max_imgs]\n",
    "\n",
    "\n",
    "print(\"Collecting real images for FID evaluation...\")\n",
    "real_imgs = get_all_real_images(dataloader)\n",
    "\n",
    "print(\"Computing FID score for diffusion model...\")\n",
    "fid_score = compute_fid(real_imgs, model, noise_scheduler, device, num_samples=min(100, len(real_imgs)))\n",
    "\n",
    "print(f\"Fréchet Inception Distance (FID): {fid_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
